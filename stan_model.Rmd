---
title: "Hierarchical Bayesian Model"
author: "George"
date: "`r Sys.Date()`"
output: html_document
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
library(ggplot2)
library(bayesplot)
library(rstan)

source("scripts/stan_helpers.R")
```

## Load data

```{r load-and-prep-data, include=F}
bilingual_item_data_clean <- readRDS("data/bilingual_item_data_clean.Rds")
data_item <- bilingual_item_data_clean |> 
  filter(dataset_origin_name == "Marchman Dallas Bilingual")

pids <- unique(data_item$child_id)
# 10 kids x 680 words x 2 langs = 13600 (but somehow 15440 data points?) - 43 minutes
itids <- unique(data_item$item_id)
# 144 kids x 200 words x 2 langs = 57600 (but somehow 72851?) - 252 minutes

# full dataset (247913) should take: 3.4*252 / 60 = 14.3 hrs
data_item <- data_item |> 
  #filter(is.element(child_id, sample(pids, 10))) |>
  filter(is.element(item_id, sample(itids, 300))) |>
  select(produces, age, exposure_proportion, child_id, uni_lemma, item_id, language) |> 
  na.omit()

# Prepare data for Stan model
stan_data <- list(
  N = nrow(data_item),
  I = length(unique(data_item$child_id)),
  W = length(unique(data_item$uni_lemma)),
  J = length(unique(data_item$item_id)),
  L = length(unique(data_item$language)),
  y = data_item$produces,
  age = data_item$age,
  exposure = data_item$exposure_proportion,
  child = as.integer(factor(data_item$child_id)),
  word = as.integer(factor(data_item$uni_lemma)),
  item = as.integer(factor(data_item$item_id)),
  lang = as.integer(factor(data_item$language))
)
```


Let's try copying Alvin's GAMLSS model in Stan.

```{r, eval=F}
rstan_options(auto_write = TRUE) 
options(mc.cores = parallel::detectCores())

fit <- stan(file = "models/model2.stan",
            data = stan_data,
            model_name = "linear",
            iter = 4000, warmup = 1000, chains = 4, seed = 123)
saveRDS(fit, file="models/model2_fit.rds")
```



```{r}
# Check the results
fit <- readRDS("models/model2_fit.rds")
print(fit)
```

## Posterior Predictive Check

```{r, eval=F}
# Extract generated quantities
y_rep <- extract(fit)$y_rep

# For example, compute the posterior predictive mean for each observation
y_rep_mean <- apply(y_rep, 2, mean)

# Plot observed vs. predicted probabilities (or frequencies)
df_ppc <- data.frame(
  observed = y,
  predicted = y_rep_mean
)

ggplot(df_ppc, aes(x = predicted, fill = factor(observed))) +
  geom_histogram(position = "dodge", bins = 30) +
  labs(title = "Posterior Predictive Distribution",
       x = "Predicted probability",
       fill = "Observed (0/1)") +
  theme_minimal()
```


# Model 3: Non-linear Effect of Age


```{r}
library(splines)

# Generate a B-spline basis for age with 4 degrees of freedom
age_spline <- bs(data_item$age, df = 4)

# Add the spline basis 
data_item <- cbind(data_item, age_spline)

# Prepare the Stan data list
stan_data <- list(
  N = nrow(data_item),
  I = length(unique(data_item$child_id)),
  W = length(unique(data_item$uni_lemma)),
  J = length(unique(data_item$item_id)),
  L = length(unique(data_item$language)),
  y = data_item$produces,
  exposure = data_item$exposure_proportion,
  child = as.integer(as.factor(data_item$child_id)),
  word = as.integer(as.factor(data_item$uni_lemma)),
  item = as.integer(as.factor(data_item$item_id)),
  lang = as.integer(as.factor(data_item$language)),
  age_spline = as.matrix(age_spline)  # Pass spline basis as a matrix
)
```

```{r, eval=F}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores()) 

fit <- stan(file = "models/model3-spline.stan",
            data = stan_data,
            model_name = "spline",
            control = list(max_treedepth=12),
            iter = 2500, warmup = 1000, chains = 4, seed = 123,
            thin = 2)
saveRDS(fit, file="models/model3-spline_fit.rds")
```

```{r}
# Check the results
fit <- readRDS("models/model3-spline_fit.rds")
#print(fit)
```

## Posterior Predictive Check


```{r}
plot(fit)
```

```{r}
posterior_samples <- extract(fit)
beta_age_spline <- posterior_samples$beta_age_spline

age_range <- seq(min(data_item$age), max(data_item$age), length.out = 100)

# Create the spline basis for the age range (using the same settings as in your STAN model)
age_spline_basis <- ns(age_range, df = 4)  # 4 degrees of freedom used in your STAN model

# Calculate the predicted spline for the mean of the posterior samples
mean_beta_age_spline <- colMeans(beta_age_spline)

predicted_spline <- age_spline_basis %*% mean_beta_age_spline  # Predicted values

ggplot(data = data.frame(age_range, predicted_spline), aes(x = age_range, y = predicted_spline)) +
  geom_line(color = "blue", size = 1) +
  labs(x = "Age", y = "Predicted Spline Effect", title = "Fitted Spline for Age") +
  theme_minimal()
```

```{r}
# Generate predicted splines for each posterior sample
predicted_splines <- age_spline_basis %*% t(beta_age_spline)

# Compute the 2.5th and 97.5th percentiles for the uncertainty intervals
ci_lower <- apply(predicted_splines, 1, quantile, probs = 0.025)
ci_upper <- apply(predicted_splines, 1, quantile, probs = 0.975)

spline_df <- data.frame(age_range, predicted_spline, ci_lower, ci_upper)

# Plot with credible intervals
ggplot(spline_df, aes(x = age_range)) +
  geom_line(aes(y = predicted_spline), color = "blue", size = 1) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.2, fill = "blue") +
  labs(x = "Age", y = "Predicted Spline Effect", title = "Fitted Spline with 95% Credible Interval") +
  theme_minimal()
```

```{r}
plot_children(posterior_samples)
plot_items(posterior_samples)
```


```{r, eval=F}
# Extract generated quantities
y_rep <- extract(fit)$y_rep

# compute the posterior predictive mean for each observation
y_rep_mean <- apply(y_rep, 2, mean)

df_ppc <- data.frame(
  observed = stan_data$y,                # Actual outcomes (binary: word produced or not)
  predicted = y_rep_mean,                # Predicted probabilities
  child = stan_data$child,               # Child ID for each observation
  exposure = stan_data$exposure,
  age = data_item$age
)

df_ppc$residuals <- df_ppc$observed - df_ppc$predicted

df_vocab <- df_ppc %>%
  group_by(child) %>%
  summarize(
    total_observed_vocab = sum(observed),   # Total actual words produced by child
    total_predicted_vocab = sum(predicted), # Total predicted vocabulary size
    mean_age = mean(age),      # Average age per child (or replace with raw age if available)
    exposure = mean(exposure)
  )

# Residuals vs age plot
ggplot(df_ppc, aes(x = age, y = residuals)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "loess", color = "blue", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs. Age",
    x = "Age",
    y = "Residuals (Observed - Predicted)"
  ) +
  theme_minimal()

ggplot(df_ppc, aes(x = exposure, y = residuals)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "loess", color = "blue", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs. Exposure",
    x = "Exposure",
    y = "Residuals (Observed - Predicted)"
  ) +
  theme_minimal()
```

```{r}
# Extend df_ppc to include total observed/predicted vocab per child
df_ppc <- df_ppc %>%
  group_by(child) %>%
  mutate(
    total_observed_vocab = sum(observed),   # Total actual words produced by child
    total_predicted_vocab = sum(predicted), # Total predicted vocabulary size
    mean_age_spline = mean(age_spline)      # Average age (or use actual age if available)
  ) %>%
  ungroup()

# Posterior predictive distribution with facets for each child
ggplot(df_ppc, aes(x = predicted, fill = factor(observed))) +
  geom_histogram(position = "dodge", bins = 30) +
  facet_wrap(~ child, ncol = 5) +  # Facet by child
  labs(
    title = "Posterior Predictive Distribution by Child",
    x = "Predicted Probability",
    fill = "Observed (0/1)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


Let's specify a hierarchical Bayesian model with a shared difficulty for each concept, which underlies the words in different languages, that can allow for language‐specific deviations at the word level. 
The probability that child $i$ produces word $w$ in language $L$ is modeled as a function of the child's age, their language-specific exposure, the shared conceptual difficulty, and a word-level deviation. 

Concept Difficulty: For each concept $c$ (e.g., “dog”):

$\theta_c \sim \mathcal{N}(\mu_{\theta}, \sigma_{\theta}^2)$

Word-Level Deviation: For each word $w$ in language $L$ corresponding to concept $c$:

$\delta_{w, L} \sim \mathcal{N}(0, \sigma_{\delta}^2)$

Then the overall difficulty for word $w$ in language $L$ is:

$\beta_{w, L} = \theta_c + \delta_{w, L}$

## Child-Level Effects

Each child $i$ has an ability (i.e. latent skill level) parameter $\alpha_i$, and the effect of age can be modeled separately (or absorbed into $\alpha_i$ if age is the primary driver). 
Also, let exposure in language $L$ for child $i$ be $E_{i, L}$. 

$\eta_{i, L} = \alpha_i + \gamma \cdot \text{age}_i + \lambda \cdot E_{i, L}$

We could allow for cross-language influences by including an effect of exposure in the other language $E_{i, L'}$ with its own coefficient.

## Linking to Word Production

The probability that child $i$ produces word $w$ in language $L$ (denoted by $y_{i, w, L}$, typically 1 if produced and 0 otherwise) is given by:

$\text{Pr}(y_{i, w, L} = 1) = \text{logit}^{-1}(\eta_{i, L} - \beta_{w, L})$

Thus, higher ability, age, or exposure in a given language increases the log-odds of production, while greater word difficulty (coming from the underlying concept and word-level deviation) decreases it.

\begin{tikzpicture}
  % Plates
  \draw[thick] (-1.5, -1) rectangle (6, 2.5) node[above left] {Children \(I\)};
  \draw[thick] (0.5, -0.5) rectangle (5.5, 2) node[above left] {Words \(W\)};
  \draw[thick] (1, 0) rectangle (5, 1.5) node[above left] {Concepts \(C\)};
  
  % Nodes for child, word, concept
  \node at (-.5, .5) (alpha) {$\alpha_i$};
  \node at (-.5, 1.2) (gamma) {$\gamma$};
  \node at (1.5, 1.7) (theta) {$\theta_c$};
  \node at (3, 2.2) (delta) {$\delta_w$};
  \node at (2.75, 1.25) (beta) {$\beta_{c,w}$};

  % Exposure and language effect
  \node at (1.5, 0.3) (lambda_L1) {$\lambda_{1}$};
  \node at (4, 0.3) (lambda_L2) {$\lambda_{2}$};

  % Arrows for dependencies
  \draw[->] (alpha) -- (beta);
  \draw[->] (gamma) -- (beta);
  \draw[->] (theta) -- (beta);
  \draw[->] (delta) -- (beta);
  \draw[->] (lambda_L1) -- (beta);
  \draw[->] (lambda_L2) -- (beta);

  % Response node
  \node at (6.5, .5) (y) {$y_n$};
  \draw[->] (beta) -- (y);

  % Plates
  \draw[dashed] (-2.5, -1.5) rectangle (7, 3.5) node[above left] {Observations \(N\)};
\end{tikzpicture}


## Generate data

```{r generate-data}
set.seed(123)

# Define dimensions
I <- 100 # number of children
W <- 100 # number of words
C <- 100 # number of concepts
L <- 2   # two languages
N <- I * W * L # total number of observations

# Simulate mappings:
child <- rep(1:I, each = W * L)  # Each child takes 100 words in 2 languages
word <- rep(rep(1:W, each = L), times = I)  # Same words for each child in both languages
concept <- rep(1:C, L)  # Each word is mapped to a unique concept, same concept for both languages
lang <- rep(1:L, times = I * W)  # Language indicator (1 for L1, 2 for L2)
# Assuming W = 200 words (100 words per language), and C = 100 concepts:

#lang <- rep(1:L, each = C)  # Language indicator for each word, 1 for words 1-100 and 2 for words 101-200

# Simulate age and exposure data
age <- runif(I, 12, 36)  # ages in months for each child
age <- rep(age, each = W * L)  # replicate age for each observation per child

exposure_L1 <- runif(I, 0.1, 0.9)  # exposure to L1 for each child
exposure_L2 <- 1 - exposure_L1  # exposure to L2 is complementary
exposure <- cbind(exposure_L1, exposure_L2)
exposure <- exposure[rep(1:I, each = W * L), ]  # replicate exposure for each child

# Generate true parameters:
mu_theta_true <- 0.0
sigma_theta_true <- 1.0
theta_true <- rnorm(C, mu_theta_true, sigma_theta_true)  # concept difficulties

sigma_delta_true <- 0.5
delta_true <- rnorm(W, 0, sigma_delta_true)  # word-specific deviations

alpha_true <- rnorm(I, 0, 1)  # child-specific random effects
alpha_true <- rep(alpha_true, each = W * L)  # replicate for all observations

gamma_true <- 0.05  # age effect
lambda_true <- c(1.0, 0.8)  # language exposure effects for L1 and L2

# Generate probabilities and simulated responses
eta <- numeric(N)
for (n in 1:N) {
  w <- word[n]  # word index
  c <- concept[w]  # concept index
  l <- lang[n]  # language index
  beta <- theta_true[c] + delta_true[w]  # combined concept and word effect
  eta[n] <- alpha_true[n] + gamma_true * age[n] + lambda_true[l] * exposure[n, l] - beta  # linear predictor
}
prob <- plogis(eta)  # convert to probabilities using logistic function
y <- rbinom(N, size = 1, prob = prob)  # binary responses (whether word is produced)

# Package data for Stan
stan_data <- list(
  N = N,
  I = I,
  W = W,
  C = C,
  L = L,
  child = child,
  word = word,
  concept = concept,
  lang = lang,
  age = age,
  exposure = exposure,
  y = y
)
```

# Model fitting

```{r, eval=F}
# Compile and fit the model with rstan
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

fit <- stan(file = "models/model1.stan",
            model_name = "baseline",
            data = stan_data,
            iter = 2000, warmup = 1000, chains = 4, seed = 123)

print(fit, pars = c("mu_theta", "sigma_theta", "sigma_delta", "gamma", "lambda"))
```

# Posterior Predictive Check

```{r, eval=F}
# Extract generated quantities
y_rep <- extract(fit)$y_rep

# For example, compute the posterior predictive mean for each observation
y_rep_mean <- apply(y_rep, 2, mean)

# Plot observed vs. predicted probabilities (or frequencies)
df_ppc <- data.frame(
  observed = y,
  predicted = y_rep_mean
)

ggplot(df_ppc, aes(x = predicted, fill = factor(observed))) +
  geom_histogram(position = "dodge", bins = 30) +
  labs(title = "Posterior Predictive Distribution",
       x = "Predicted probability",
       fill = "Observed (0/1)") +
  theme_minimal()
```


```{r, eval=F}
#  plot a PPC using bayesplot
ppc_dens_overlay(y, y_rep[1:1000,])  # overlay density plots for a subset of replications
```


```{r, eval=F}
# Merge y_rep with the original data
data_merged <- data.frame(
  child = stan_data$child,
  word = stan_data$word,
  age = stan_data$age,
  lang = stan_data$lang,  # 1 = L1, 2 = L2
  exposure_L1 = stan_data$exposure[stan_data$child, 1],  # L1 exposure for each child
  exposure_L2 = stan_data$exposure[stan_data$child, 2],  # L2 exposure for each child
  y_obs = stan_data$y,  # original observed data
  y_pred = y_rep_mean  # posterior predictive mean
)
```

```{r, eval=F}
# Summarize total vocabulary size per child, by language
vocab_summary <- data_merged |>
  group_by(child, age, lang, exposure_L1, exposure_L2) |>
  summarise(
    total_vocab_pred = sum(y_pred),
    total_vocab_obs = sum(y_obs)
  )

# Separate summary for L1 and L2
vocab_summary_L1 <- filter(vocab_summary, lang == 1)
vocab_summary_L2 <- filter(vocab_summary, lang == 2)
```

```{r, eval=F}
# Plot predicted total vocabulary in L1 by age and L1 exposure
ggplot(vocab_summary_L1, aes(x = age, y = total_vocab_pred, color = exposure_L1)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE) +  # add a trend line
  scale_color_gradient(low = "blue", high = "red") +  # exposure scale
  labs(title = "Predicted Total Vocabulary in L1 by Age and Exposure",
       x = "Age (months)", y = "Predicted Total Vocabulary (L1)",
       color = "L1 Exposure") +
  theme_minimal()

# Plot predicted total vocabulary in L2 by age and L2 exposure
ggplot(vocab_summary_L2, aes(x = age, y = total_vocab_pred, color = exposure_L2)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE) +  # add a trend line
  scale_color_gradient(low = "blue", high = "red") +  # exposure scale
  labs(title = "Predicted Total Vocabulary in L2 by Age and Exposure",
       x = "Age (months)", y = "Predicted Total Vocabulary (L2)",
       color = "L2 Exposure") +
  theme_minimal()

# Optionally, combine observed and predicted vocabulary into the same plot

ggplot(vocab_summary_L1, aes(x = age)) +
  geom_point(aes(y = total_vocab_pred, color = "Predicted"), size = 2) +
  geom_point(aes(y = total_vocab_obs, color = "Observed"), shape = 1, size = 2) +
  geom_smooth(aes(y = total_vocab_pred, color = "Predicted"), method = "loess", se = FALSE) +
  geom_smooth(aes(y = total_vocab_obs, color = "Observed"), method = "loess", se = FALSE, linetype = "dashed") +
  scale_color_manual(values = c("Predicted" = "red", "Observed" = "blue")) +
  labs(title = "Predicted vs Observed Total Vocabulary (L1) by Age",
       x = "Age (months)", y = "Vocabulary Size (L1)",
       color = "Vocabulary") +
  theme_minimal()
```



